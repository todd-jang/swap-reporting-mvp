# deployment/kubernetes/tainon.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: tainon-deployment
  labels:
    app: tainon
spec:
  replicas: 3 # 실시간 부하에 따라 Pod 인스턴스 수 (HPA로 자동 조절 가능)
  selector:
    matchLabels:
      app: tainon
  template:
    metadata:
      labels:
        app: tainon
    spec:
      # 높은 우선순위 설정 (PriorityClass가 정의되어 있어야 함)
      # priorityClassName: high-priority-realtime # 예시 PriorityClass 이름

      containers:
      - name: tainon
        image: your_docker_registry/tainon:latest # TainOn 서비스 Docker 이미지 경로
        ports:
        - containerPort: 8002 # 실시간 데이터 리스너 포트 (예시)
        env:
        - name: AI_INFERENCE_API_URL # AI Inference Service API URL 환경 변수
          value: http://ai-inference-service:8001/predict_anomaly # Kubernetes Service Name 사용

        resources:
          requests: # 최소 요청 자원 (Guaranteed QoS를 위해 limit와 동일하게 설정 권장)
            cpu: "1"
            memory: "2Gi"
            # AI Inference Service를 별도로 호출하는 방식이므로 TainOn Pod 자체는 GPU를 요청하지 않음
            # nvidia.com/gpu: "0" # 명시적으로 0으로 설정 가능

          limits: # 최대 사용 제한 자원 (Guaranteed QoS를 위해 request와 동일하게 설정 권장)
            cpu: "1"
            memory: "2Gi"
            # AI Inference Service 호출 방식이므로 TainOn Pod 자체는 GPU를 사용하지 않음
            # nvidia.com/gpu: "0"

        # TODO: Readiness Probe, Liveness Probe 설정
        # readinessProbe:
        #   httpGet:
        #     path: /healthz # TainOn 서비스 헬스체크 엔드포인트 (구현 필요)
        #     port: 8002
        #   initialDelaySeconds: 5
        #   periodSeconds: 10

      # 실시간 처리에 적합한 노드에 배치 (선택 사항)
      # nodeSelector:
      #   node-role.kubernetes.io/worker: "" # 워커 노드에만 배포 예시

---
# TainOn 서비스를 위한 Service (외부 노출보다는 클러스터 내부 통신용)
# 만약 외부에서 직접 TainOn 리스너로 데이터를 푸시한다면 NodePort 또는 LoadBalancer 타입 고려
# apiVersion: v1
# kind: Service
# metadata:
#   name: tainon-service
# spec:
#   selector:
#     app: tainon
#   ports:
#   - protocol: TCP
#     port: 8002 # Service 포트
#     targetPort: 8002 # Pod 컨테이너 포트
