테스트 실행 방법:

가상 구성 요소 모듈 파일(vb_api_svr.py 등)을 프로젝트에 추가합니다. 실제 구현체로 교체해야 합니다.

tests/ 디렉터리 안에 생성된 테스트 파일들을 추가합니다.

프로젝트 루트 디렉터리에서 터미널을 열고 다음 명령어를 실행하여 pytest 테스트를 실행합니다.

Bash

pytest
또는 특정 파일만 실행:

Bash

pytest tests/test_unit.py
pytest tests/test_integration.py
pytest tests/test_data_process.py
Selenium 테스트를 실행하려면 웹 서버(TainWeb)가 실행 중이어야 합니다.

Selenium 테스트를 위해 ChromeDriver가 필요합니다. webdriver-manager를 사용하면 자동으로 관리됩니다. --headless 옵션을 사용하지 않으면 브라우저 창이 열립니다.


테스트 완성 지침:

실제 코드 및 경로 반영: 위 코드의 TODO 부분과 가상 모듈/함수 호출 부분을 여러분의 "swap-reporting-mvp" 프로젝트의 실제 파일 경로, 모듈 이름, 함수/클래스 이름으로 모두 교체해야 합니다.
CFTC 가이드라인 구체화: test_data_process.py 파일의 각 테스트 케이스는 제공된 PDF 스니펫과 전체 가이드라인 문서를 기반으로, 어떤 입력 데이터에 대해 어떤 정확한 출력(파싱 결과, 변환 결과, 집계 값, 오류 유형 등)을 기대하는지 정확하게 정의하고 이를 assert 문으로 검증해야 합니다.
아키텍처 흐름 구현 및 검증: test_integration.py 파일에서는 아키텍처 다이어그램의 화살표로 표현된 구성 요소 간의 데이터 또는 제어 흐름을 테스트 코드로 구현합니다. 한 구성 요소의 출력이 다음 구성 요소의 입력이 되는 과정을 테스트하고, 각 단계에서 예상되는 중간 결과나 최종 결과가 올바른지 검증합니다. 모의 객체를 사용하여 시스템의 특정 부분만 테스트 범위에 포함시킬 수 있습니다.
데이터 무결성 검증: 파이프라인의 여러 단계를 거친 데이터가 원본 데이터와 비교하여 유실, 변조, 오류 없이 정확하게 변환 및 집계되었는지 검증하는 테스트를 포함합니다.
다양한 시나리오 커버: 정상적인 데이터 흐름뿐만 아니라, 유효하지 않은 데이터 입력 시 파이프라인 각 단계에서 어떻게 처리되는지, 외부 서비스(DB, 파일 시스템 등) 오류 발생 시 어떻게 동작하는지 등의 예외 시나리오도 테스트합니다.
테스트 데이터 준비: 각 테스트 케이스에 필요한 입력 데이터(파일 내용, DB 레코드 초기 상태, API 요청 페이로드)를 정교하게 준비하고 관리합니다.


각 테스트 파일 플레이스홀더 채우는 방법:

1. tests/test_unit.py (단위 테스트)
목표: 여러분 프로젝트의 각 구성 요소(Vb api svr, TainTube, TainBat 등) 내의 개별 함수나 메소드가 예상대로 작동하는지 격리하여 테스트합니다.
작성 방법:
테스트할 함수나 메소드가 속한 모듈을 정확하게 임포트합니다. (예: from your_module_path import your_function_name)
테스트 대상 함수/메소드가 의존하는 외부 요소(다른 함수, 클래스 인스턴스, 외부 라이브러리 호출 등)는 pytest-mock의 mocker.patch()를 사용하여 모의(Mock) 객체로 대체합니다. 이를 통해 테스트 대상 코드 자체의 로직만 테스트합니다.
테스트 케이스별로:
입력 값을 준비합니다.
모의 객체가 반환해야 할 값이나 발생해야 할 예외를 설정합니다.
테스트 대상 함수/메소드를 호출합니다.
assert 문을 사용하여 함수의 반환 값이 예상과 일치하는지, 예상되는 예외가 발생하는지, 모의 객체가 예상대로 호출되었는지 등을 검증합니다.
예시 채우기: 앞서 test_unit.py에 있는 인증 로직, 데이터 파싱, 데이터 처리 함수의 단위 테스트 예시를 참고하여, 여러분의 실제 함수 이름, 입력/출력 데이터 구조, 발생 가능한 예외 등에 맞춰 코드를 작성합니다. 예를 들어:
TainTube의 데이터 파싱 함수 테스트 시, 실제 CFTC 입력 라인 포맷에 맞는 문자열을 입력으로 주고, 여러분의 파싱 함수가 반환하는 실제 데이터 구조(딕셔너리 등)와 일치하는지 검증합니다. 잘못된 포맷의 라인을 입력으로 주고 파싱 함수가 올바른 예외를 발생시키는지도 테스트합니다.
TainBat의 데이터 변환 함수 테스트 시, CFTC 가이드라인에 따른 원본 데이터 구조(딕셔너리)를 입력으로 주고, 변환된 데이터 구조와 값이 가이드라인의 규칙(예: 통화 변환 후 가격)에 따라 정확한지 검증합니다.
2. tests/test_integration.py (통합 테스트)
목표: 아키텍처 다이어그램(IMG_2008.jpg) 상의 두 개 이상의 구성 요소나 시스템 경계(API, 파일, DB) 간의 상호작용이 예상대로 작동하는지 테스트합니다.
작성 방법:
테스트 대상 구성 요소들을 임포트합니다.
시스템의 외부 경계(실제 외부 API, 실제 외부 DB 서비스 등)는 필요한 경우 mocker.patch()로 모의할 수 있습니다. 하지만 내부 구성 요소 간의 연동은 가능한 실제 구현체를 사용합니다.
API 연동 테스트: pytest-django와 Django 테스트 클라이언트를 사용하거나, requests 라이브러리 등을 사용하여 여러분의 Vb/Vo API 서버 엔드포인트에 실제 HTTP 요청을 보냅니다. 요청 헤더(인증 토큰 포함), 요청 바디, URL 파라미터 등을 정확히 설정하고, 응답 상태 코드, 응답 바디 내용, 헤더 등을 검증합니다. 인증 실패, 권한 부족 등 다양한 시나리오를 테스트합니다.
데이터 파이프라인 연동 테스트:
파일 시스템 연동 테스트 시: tempfile 모듈 등을 사용하여 테스트용 임시 파일을 생성하고 읽고 쓰는 작업을 시뮬레이션합니다. 파일 경로가 올바르게 전달되고 내용이 정확한지 검증합니다.
데이터베이스 연동 테스트 시: pytest.mark.django_db 마커(Django 사용 시)를 사용하거나, 테스트용 인메모리 DB 또는 별도 테스트 DB 인스턴스를 설정합니다. 구성 요소가 DB에 데이터를 올바르게 쓰고(INSERT/UPDATE), 올바르게 읽어오는지(SELECT) 검증합니다.
구성 요소 간 직접 호출 테스트: 특정 구성 요소의 함수/메소드가 다른 구성 요소의 함수/메소드를 호출하는 경우, 호출하는 쪽에서 피호출하는 쪽의 함수/메소드를 직접 호출하고 입력/출력 및 상태 변화를 검증합니다. 피호출하는 메소드가 또 다른 외부 의존성을 가진다면 해당 부분은 모의할 수 있습니다.
예시 채우기:
TainTube의 process_incoming_data_file 함수를 호출하고, 이 함수가 실행된 후 예상되는 데이터가 실제 DB(테스트 DB)에 저장되었는지 또는 특정 임시 파일에 기록되었는지 assert 문으로 검증합니다.
Vo API 서버의 trigger_report_generation API 엔드포인트에 POST 요청을 보내고, 응답 상태 코드(예: 202 Accepted)와 응답 바디(보고서 ID 등)를 검증합니다. 이 API가 내부적으로 TainBat 또는 TainOn의 특정 메소드를 호출한다면, 해당 내부 호출이 발생했는지 mocker.patch를 사용하여 확인하는 테스트도 추가할 수 있습니다.
3. tests/test_data_process.py (데이터 유효성 및 처리 테스트)
목표: CFTC 가이드라인에 명시된 데이터 유효성 검증 규칙, 데이터 변환 규칙, 데이터 집계 방식, 보고서 형식 및 내용이 여러분의 코드로 정확하게 구현되었는지 테스트합니다.
작성 방법:
CFTC 가이드라인 문서를 옆에 두고 각 규칙별로 테스트 케이스를 작성합니다.
pytest.fixture를 사용하여 가이드라인에 맞는 다양한 유효/유효하지 않은 테스트 데이터셋을 구조화하여 정의합니다. PDF 스니펫의 변동 증거금, 통화, 포트폴리오/거래 단위 정보 등을 반영하여 실제 데이터를 생성합니다.
유효성 검증 테스트: cftc_validate_record와 같은 유효성 검증 함수에 유효한 데이터를 입력으로 주고 True를 반환하거나 예외가 발생하지 않는지 확인합니다. 유효하지 않은 데이터를 입력으로 주고 예상되는 ValidationError (또는 여러분이 정의한 다른 예외)가 발생하는지, 그리고 오류 메시지가 가이드라인에 부합하는지(필요하다면) 검증합니다.
변환 테스트: apply_cftc_transformations와 같은 데이터 변환 함수에 원본 데이터를 입력으로 주고, 변환 결과 데이터가 가이드라인에 명시된 형태(데이터 타입, 값 변경, 파생 필드 생성 등)로 정확하게 변환되었는지 검증합니다. 통화 변환 로직이 있다면 환율 정보를 모의하거나 고정하여 정확한 변환 결과 값을 검증합니다.
집계 테스트: aggregate_trades와 같은 집계 함수에 여러 레코드로 구성된 데이터 리스트를 입력으로 주고, 가이드라인에 명시된 기준(날짜별, 주체별, 포트폴리오별 등)과 계산 방식(합계, 평균, 건수 등)에 따라 정확하게 집계된 결과가 반환되는지 검증합니다.
보고서 생성 테스트: generate_cftc_report_document와 같은 보고서 생성 함수에 처리/집계된 데이터를 입력으로 주고, 생성된 보고서 파일(또는 데이터 구조)의 형식, 포함된 필드, 계산된 최종 값 등이 CFTC 가이드라인에 명시된 보고서 형식과 내용에 정확히 부합하는지 검증합니다. 복잡한 보고서의 경우, 특정 필드 값, 행/열의 순서, 파일 인코딩 등을 꼼꼼히 확인합니다.
데이터 흐름 정확성 테스트 (심화): 입력 파일 내용 -> 파싱 -> 검증 -> 변환 -> 집계 -> 보고서 최종 데이터까지 각 단계를 거친 데이터가 정확하게 다음 단계로 전달되고 최종 결과가 올바른지 확인하는 통합 테스트를 test_data_process.py 또는 test_integration.py에 작성할 수 있습니다.
4. tests/test_e2e_ui.py (Selenium 테스트)
목표: 웹 브라우저를 통해 "TainWeb" UI의 사용자 흐름 전체가 정상적으로 작동하는지 테스트합니다.
작성 방법:
browser fixture를 사용하여 Selenium WebDriver 인스턴스를 확보합니다.
browser.get() 메소드로 페이지에 접속합니다.
browser.find_element() 메소드와 By 전략(ID, XPATH, CSS Selector 등)을 사용하여 UI 요소를 찾습니다.
send_keys()로 입력 필드에 값을 입력하고, click()으로 버튼을 클릭합니다.
WebDriverWait와 expected_conditions를 사용하여 특정 요소가 나타날 때까지 대기하는 등 페이지 로딩이나 JavaScript 실행을 기다립니다.
assert 문을 사용하여 페이지 제목, 특정 요소의 존재 여부, 요소의 텍스트나 속성 값 등을 검증합니다.
예시 채우기: 앞선 test_e2e_ui.py 예시를 참고하여, 실제 "TainWeb"의 로그인 페이지 URL, 입력 필드 ID/이름, 버튼 요소 선택자, 로그인 후 이동하는 페이지의 요소 ID/클래스/텍스트 등을 여러분의 프론트엔드 코드에 맞게 수정합니다. 관리자 로그인, 보고서 페이지 접근, 보고서 필터 적용, 보고서 다운로드 등 핵심 사용자 흐름을 시나리오로 만듭니다.
5. tests/test_performance.py (성능 테스트)
목표: 코드 레벨에서 특정 로직의 처리 시간을 측정하거나, 통합된 흐름의 응답 시간을 가볍게 측정합니다. 종합적인 부하 테스트는 외부 도구로 수행합니다.
작성 방법:
time 모듈을 사용하여 코드 블록의 시작과 끝 시간을 기록하고 차이를 계산하여 소요 시간을 측정합니다.
테스트할 함수나 메소드를 여러 번 반복 호출하여 평균 시간을 측정합니다.
API 엔드포인트의 성능을 측정하려면 pytest-django의 테스트 클라이언트(Client)를 사용하거나 requests 라이브러리를 사용하여 API 호출 시간을 측정합니다.
성능 측정 시 외부 의존성(DB, 외부 API)은 실제 성능에 큰 영향을 미치므로, 필요에 따라 모의하거나 테스트 DB 등을 사용하여 특정 부분의 성능만 측정할지, 아니면 통합된 성능을 측정할지 결정합니다.
assert 문을 사용하여 측정된 시간이 특정 임계값(예: 평균 처리 시간 100ms 미만)보다 짧은지 검증합니다.
예시 채우기: 앞선 test_performance.py 예시를 참고하여, 여러분의 핵심 로직(예: 대량 데이터 파싱, 복잡한 집계 연산, 보고서 데이터베이스 조회 등)의 처리 시간을 측정하는 테스트를 추가합니다. API 성능 측정 시에는 실제 API 엔드포인트 URL을 사용합니다.
실제 코드 작성 시 저에게 도움받는 방법:

특정 함수나 메소드에 대한 단위 테스트 작성 방법을 모르겠다면, 해당 함수의 코드와 의존하는 외부 요소(호출하는 다른 함수, 사용하는 객체 등) 정보를 저에게 공유해주세요. 해당 함수를 어떻게 모의하고 테스트할 수 있는지 구체적인 pytest 코드를 함께 작성해 드릴 수 있습니다.
두 구성 요소 간의 연동 테스트를 작성하는 데 어려움이 있다면, 관련된 두 구성 요소의 코드(API 엔드포인트 핸들러, 데이터 처리 메소드 등)와 이들이 서로를 어떻게 호출하는지에 대한 정보를 공유해주세요. 이를 바탕으로 통합 테스트 시나리오와 코드를 함께 만들어 갈 수 있습니다.
CFTC 가이드라인의 특정 규칙(예: "필드 X는 조건부 필수이며, 필드 Y가 Z 값일 때만 필요하고 형식은 날짜YYYYMMDD이다")에 대한 테스트 코드를 어떻게 작성해야 할지 모르겠다면, 해당 규칙의 명세와 이를 처리하는 여러분의 코드 부분을 공유해주세요. 규칙을 검증하는 구체적인 테스트 케이스를 설계하고 코드화하는 것을 도와드리겠습니다

테스트 코드 완성 프로세스:

실제 코드 임포트: 각 테스트 파일 상단에서 swap_reporting_mvp 프로젝트의 실제 모듈 및 함수를 정확한 경로로 임포트합니다.
가상 객체/함수 제거 및 실제 코드 사용: 플레이스홀더로 남겨둔 가상 객체(MockVbApiSvrMain 등) 및 가상 함수 정의는 제거하고, mocker.patch를 사용하여 실제 모듈/함수를 모의하거나, 테스트하려는 실제 함수를 직접 호출하도록 코드를 수정합니다.
모의 설정 구체화: mocker.patch로 모의하는 각 함수/메소드가 테스트 시나리오에 따라 어떤 값을 반환해야 하는지(return_value) 또는 어떤 예외를 발생시켜야 하는지(side_effect)를 구체적으로 설정합니다.
assert 문 구현: 각 TODO 부분에 있는 assert 문을 여러분의 실제 코드의 반환 값, 호출 인자, 상태 변화 등을 검증하도록 구체화합니다. pytest의 다양한 assert 구문(예: assert actual == expected, pytest.raises(...), mock_object.assert_called_once_with(...), mock_object.assert_has_calls(...))을 활용합니다.
테스트 데이터 준비: 각 테스트 케이스 실행에 필요한 입력 데이터(파일 내용, DB 초기 상태, API 요청 바디 등)를 구체적인 값으로 정의합니다. pytest.fixture를 사용하면 테스트 데이터 관리가 용이합니다.
CFTC 가이드라인 반영: test_data_process.py 파일은 특히 CFTC 가이드라인의 상세 규칙에 맞는 입력 데이터와 예상 결과 값을 정의하고 이를 검증하는 데 집중합니다.
아키텍처 흐름 추적: test_integration.py에서 각 테스트 케이스는 아키텍처 다이어그램 상의 특정 데이터 또는 제어 흐름을 따라가면서, 각 구성 요소의 호출이 올바르게 이루어지고 데이터가 예상대로 전달되는지 검증합니다.
